

ISO : ubuntu-16.04.5-desktop-amd64.iso

HostNetwrokOnlyNetwork: 172.18.18.0/24

/etc/hots

# kubernetes cluster
172.18.18.20 kubemaster.test.com kubemaster
172.18.18.21 kubeworker1.test.com worker1
172.18.18.22 kubewroker2.test.com worker2



//INSTALL kubernetes
//
URL:
https://kubernetes.io/docs/setup/independent/install-kubeadm/#verify-the-mac-address-and-product-uuid-are-unique-for-every-node
//

sudo su -

apt-get update && apt-get install -y apt-transport-https curl
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -

cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF

apt-get update
apt-get install -y kubelet kubeadm kubectl
apt-mark hold kubelet kubeadm kubectl

//////////////////////////////////////////////////////////////

https://developers.caffeina.com/a-kubernetes-cluster-on-virtualbox-20d64666a678

//////////////////////////////////////////////////////////////


Configuration.

Some other things are required to make the system work. First of all the swap must be disabled. To do that, first get the swap partition ID like this.

$ cat /proc/swaps

Then run swapoff -a to actually turn it off. 
And finally, remove the swap record from /etc/fstab

///////////////////////////////////////////////////////////////////


Secondly, both Kubernetes and Docker must have the same cgroup and that can be either cgroupfs or systemd I tried cgroupfs first but then switched to systemd

Use the command below to check which cgroup docker is using

$ docker info | grep -i cgroup

    Docker may not be configured at all

And this other one for kubernetes

$ cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

To update both to systemd execute the following

$ sed -i "s/\$KUBELET_EXTRA_ARGS/\$KUBELET_EXTRA_ARGS\ --cgroup-driver=systemd/g" /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

$ cat <<EOF >/etc/docker/daemon.json
{ 
    "exec-opts": ["native.cgroupdriver=systemd"]
}
EOF

In the end reload Docker and Kubernetes or just reboot the machine

$ systemctl daemon-reload
$ systemctl restart kubelet
$ systemctl restart docker


//////// Aqui se puede clonar tantas veces como workers  se quiera


////ARRANCAMOS K8s en master

///Salimos de usuario root y ejecutamos como otro usuario normal
sudo kubeadm init --apiserver-advertise-address=172.18.18.20 --pod-network-cidr=192.168.0.0/16 

copiamos el comando de join que devuelve

sudo kubeadm join 172.18.18.20:6443 --token cfhqvx.7k5gw6nlpshtrl9j --discovery-token-ca-cert-hash sha256:71ef1b907542459ea3a923e6a79d3780fc1b7b4197a8f43f47c60e450bddf617



////Creamos el .config
$ mkdir -p $HOME/.kube
$ sudo cp -if /etc/kubernetes/admin.conf $HOME/.kube/config
$ sudo chown $(id -u):$(id -g) $HOME/.kube/config


///You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

elegimos la opción de cálico

https://docs.projectcalico.org/v3.3/getting-started/kubernetes/



///Instalación de Cálico/////////////////
////////////////////////////////////////

kubemaster@KUBEMASTER:~$ sudo kubectl apply -f \
> https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/etcd.yaml
daemonset.extensions/calico-etcd created
service/calico-etcd created

kubemaster@KUBEMASTER:~$ sudo kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/rbac.yaml
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created


wget https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/calico.yaml

modificar la seccion #Auto-detect the BGP IP address. y que quede como esta
            # Auto-detect the BGP IP address.
            - name: IP
              value: "autodetect"
            - name: IP_AUTODETECTION_METHOD
              value: "interface=enp0s3"  ##interfaz de la red HostOnlyInterface definida



kubectl apply -f calico.yaml

kubectl taint nodes --all node-role.kubernetes.io/master

///Sacar core-dns-* de estado CrashLoopBack

kubectl edit cm coredns -n kube-system


"replacing proxy . /etc/resolv.conf with the ip address of your upstream DNS, for example proxy . 8.8.8.8." According to the link in the output of the coredns log (at the end of the page)
["8.8.8.8", "8.8.4.4", "159.107.3.32", "147.117.20.200", "153.88.112.200"]

Por ejejmplo:
proxy . 159.107.3.32

save and exit.

kubectl get pods -n kube-system -oname |grep coredns |xargs kubectl delete -n kube-system

///Ver que está todo runnning
watch kubectl get pods --all-namespaces

kubemaster@KUBEMASTER:~$ kubectl get pods --all-namespaces
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
kube-system   calico-etcd-fqcmt                          1/1     Running   0          11m
kube-system   calico-kube-controllers-56985cbb56-tfb5k   1/1     Running   0          11m
kube-system   calico-node-kk27m                          2/2     Running   1          11m
kube-system   coredns-576cbf47c7-gjcnv                   1/1     Running   0          2m18s
kube-system   coredns-576cbf47c7-kkml6                   1/1     Running   0          2m18s
kube-system   etcd-kubemaster                            1/1     Running   0          11m
kube-system   kube-apiserver-kubemaster                  1/1     Running   0          11m
kube-system   kube-controller-manager-kubemaster         1/1     Running   0          11m
kube-system   kube-proxy-jshfc                           1/1     Running   0          29m
kube-system   kube-scheduler-kubemaster                  1/1     Running   0          11m

///Ver que está todo ready
kubectl get nodes -o wide

kubemaster@KUBEMASTER:~$ kubectl get nodes -o wide
NAME         STATUS   ROLES    AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
kubemaster   Ready    master   28m   v1.12.2   172.18.18.20   <none>        Ubuntu 16.04.5 LTS   4.15.0-36-generic   docker://18.6.1




Desde cada worker tirar el comando de join y ve que se crea un nuevo calico per node

sudo kubeadm join 172.18.18.20:6443 --token cfhqvx.7k5gw6nlpshtrl9j --discovery-token-ca-cert-hash sha256:71ef1b907542459ea3a923e6a79d3780fc1b7b4197a8f43f47c60e450bddf617


kubemaster@KUBEMASTER:~$ kubectl get pods --all-namespaces
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
kube-system   calico-etcd-fqcmt                          1/1     Running   0          16m
kube-system   calico-kube-controllers-56985cbb56-tfb5k   1/1     Running   0          17m
kube-system   calico-node-c65f5                          2/2     Running   0          43s
kube-system   calico-node-kk27m                          2/2     Running   1          17m
kube-system   calico-node-rh7rz                          2/2     Running   0          24s
kube-system   coredns-576cbf47c7-gjcnv                   1/1     Running   0          7m26s
kube-system   coredns-576cbf47c7-kkml6                   1/1     Running   0          7m26s
kube-system   etcd-kubemaster                            1/1     Running   0          16m
kube-system   kube-apiserver-kubemaster                  1/1     Running   0          16m
kube-system   kube-controller-manager-kubemaster         1/1     Running   0          16m
kube-system   kube-proxy-jshfc                           1/1     Running   0          34m
kube-system   kube-proxy-l7j7x                           1/1     Running   0          43s
kube-system   kube-proxy-pfwbq                           1/1     Running   0          24s
kube-system   kube-scheduler-kubemaster                  1/1     Running   0          16m



///Set Internal IP in each node (master/worker)

Edit file in each node
/etc/systemd/system/kubelet.service.d/10-kubeadm.conf

and add --node-ip=<internal ip address node>

sudo systemctl daemon-reload
sudo systemctl restart kubelet


reset the workers

the ouput should be something like this (all nodes in the same net)

kubemaster@KUBEMASTER:~$ kubectl get nodes -o wide
NAME         STATUS   ROLES    AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
kubemaster   Ready    master   21h   v1.12.2   172.18.18.20   <none>        Ubuntu 16.04.5 LTS   4.15.0-36-generic   docker://18.6.1
worker1      Ready    <none>   20h   v1.12.2   172.18.18.21   <none>        Ubuntu 16.04.5 LTS   4.15.0-36-generic   docker://18.6.1
worker2      Ready    <none>   20h   v1.12.2   172.18.18.22   <none>        Ubuntu 16.04.5 LTS   4.15.0-36-generic   docker://18.6.1



***************************************************************************************************************************************
***************************************************************************************************************************************
***************************************************************************************************************************************

// Print join command in case of loss
// Run on Kubemaster node
kubeadm token create --print-join-command


// Install Helm in Kubemaster
apt-get install socat
curl -LO https://storage.googleapis.com/kubernetes-helm/helm-v2.8.2-linux-amd64.tar.gz

sudo tar -xvzf ./helm-v2.8.2-linux-amd64.tar.gz

sudo mv ./linux-amd64/helm /usr/local/bin/helm

cat <<EOF >  rbac-config.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
EOF

kubectl create -f rbac-config.yaml

helm init

kubectl patch deploy --namespace kube-system tiller-deploy -p '{"spec":{"template":{"spec":{"serviceAccount":"tiller"}}}}'

***************************************************************************************************************************************
***************************************************************************************************************************************
***************************************************************************************************************************************

Install Istio
 :~$ wget https://arm.lmera.ericsson.se/artifactory/proj-5g-udm-dev-helm-local/istio-1.0.1-eric-0.1.8.tgz
 :~$ tar xvzf istio-1.0.1-eric-0.1.8.tgz

Review mem values of istio-pilot in
values.yaml 
values-istio-1.0.1-rel.yaml
values-dropB.yaml

maybe 2048m is too much

 :~$ kubectl apply -f istio/templates/crds.yaml
 
 :~$ helm install --name istio --namespace istio-system istio/ \
        --set gateways.eric-udm-traffic.enabled=true \
        --set gateways.eric-udm-traffic.ports[0].port=82 \
        --set gateways.eric-udm-traffic.ports[0].targetPort=82 \
        --set gateways.eric-udm-traffic.ports[0].nodePort=31382


Output after this step
# Checking installation
 > kubectl get pods --all-namespaces
NAMESPACE      NAME                                       READY     STATUS    RESTARTS   AGE
istio-system   eric-udm-traffic-2wn66                     1/1       Running   0          1m
istio-system   eric-udm-traffic-ptgnm                     1/1       Running   0          1m
istio-system   istio-citadel-69c6c8987-84cqz              1/1       Running   0          1m
istio-system   istio-pilot-7fd44968f8-rv9bb               2/2       Running   0          1m
istio-system   istio-sidecar-injector-549445c558-65267    1/1       Running   0          1m
kube-system    calico-etcd-fqcmt                          1/1       Running   2          21h
kube-system    calico-kube-controllers-56985cbb56-tfb5k   1/1       Running   3          21h
kube-system    calico-node-c65f5                          2/2       Running   2          21h
kube-system    calico-node-kk27m                          2/2       Running   6          21h
kube-system    calico-node-rh7rz                          2/2       Running   2          21h
kube-system    coredns-576cbf47c7-gjcnv                   1/1       Running   2          21h
kube-system    coredns-576cbf47c7-kkml6                   1/1       Running   2          21h
kube-system    etcd-kubemaster                            1/1       Running   2          21h
kube-system    kube-apiserver-kubemaster                  1/1       Running   2          21h
kube-system    kube-controller-manager-kubemaster         1/1       Running   2          21h
kube-system    kube-proxy-jshfc                           1/1       Running   2          22h
kube-system    kube-proxy-l7j7x                           1/1       Running   1          21h
kube-system    kube-proxy-pfwbq                           1/1       Running   1          21h
kube-system    kube-scheduler-kubemaster                  1/1       Running   2          21h
kube-system    tiller-deploy-66bb7787c4-4zp7g             1/1       Running   0          36m

kubectl -n istio-system get deployment -listio=sidecar-injector
kubectl get namespace -L istio-injection

# Enabling automatic istio-injectio in default namespace
# Optional
kubectl label namespace default istio-injection=enabled
kubectl get namespace -L istio-injection

kubectl apply -f samples/sleep/sleep.yaml 
# Verify there are two containers 2/2
kubectl get pods
 
# Remove the pod
kubectl delete -f samples/sleep/sleep.yaml 

//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
///OJO SOLO PARA ISTIO
# Create webhook for automatic sidecar injection
# https://istio.io/docs/setup/kubernetes/sidecar-injection.html
curl -LO https://github.com/istio/istio/releases/download/0.7.1/istio-0.7.1-linux.tar.gz
tar -xvzf istio-0.7.1-linux.tar.gz 
cd istio-0.7.1
./install/kubernetes/webhook-create-signed-cert.sh --service istio-sidecar-injector --namespace istio-system --secret sidecar-injector-certs
kubectl apply -f install/kubernetes/istio-sidecar-injector-configmap-release.yaml
cat install/kubernetes/istio-sidecar-injector.yaml | ./install/kubernetes/webhook-patch-ca-bundle.sh >  install/kubernetes/istio-sidecar-injector-with-ca-bundle.yaml
kubectl apply -f install/kubernetes/istio-sidecar-injector-with-ca-bundle.yaml





////////////////////////////////////////////////////////////
https://wiki.lmera.ericsson.se/wiki/UDM_5G_TIGER_TEAM/Istio_1.0.x_helm_instructions
https://kubernetes.io/docs/reference/access-authn-authz/webhook/
sudo snap install kube-apiserver
kubectl get crd
(gateways.networking.istio.io)


////Analizar acceso desde el exterior

 1220  kubectl get virtualservices.networking.istio.io
 1224  kubectl get virtualservices.networking.istio.io -n istio-ns
 1225  kubectl describe virtualservices.networking.istio.io eric-udm-eecreatesubscription -n istio-ns
 1227  kubectl get virtualservices.networking.istio.io -n efrramo
 1228  kubectl describe virtualservices.networking.istio.io myudrsim -n efrramo
 1230  kubectl describe virtualservices.networking.istio.io myudrsim -n efrramo
 1233  kubectl describe virtualservices.networking.istio.io myudrsim -n efrramo
 1234  kubectl edit virtualservices.networking.istio.io myudrsim -n efrramo
 1235  kubectl edit virtualservices.networking.istio.io myudrsim -n efrramo
 1238  kubectl describe virtualservices.networking.istio.io myudrsim -n efrramo
 1242  kubectl describe virtualservices.networking.istio.io myudrsim -n efrramo
 1244  kubectl describe virtualservices.networking.istio.io myudrsim -n efrramo
 1248  kubectl describe virtualservices.networking.istio.io myudrsim -n efrramo
 1254  kubectl describe virtualservices.networking.istio.io myudrsim -n istio-ns
 1256  kubectl describe virtualservices.networking.istio.io 
 1257  kubectl describe virtualservices.networking.istio.io -n istio-ns
 1261  kubectl describe virtualservices.networking.istio.io -n istio-ns 
 1263  kubectl describe virtualservices.networking.istio.io -n istio-ns 
 1416  kubectl get virtualservices.networking.istio.io
 1417  kubectl get virtualservices.networking.istio.io -n istio-system
 1418  kubectl get virtualservices.networking.istio.io -n istio-ns




////////CHECK
  kubectl create namespace efrramo
  kubectl run hello-world --replicas=2 --labels="run=load-balancer-example" --image=armdocker.rnd.ericsson.se/proj_hss/5g/docker_images/udrsim:hpa1  --port=8080
  kubectl get pods --all-namespaces
  kubectl get deployments --all-namespaces	
  kubectl describe deployments hello-world
  kubectl get replicasets
  kubectl describe replicasets
  kubectl expose deployment hello-world --type=NodePort --name=example-service
  kubectl describe services example-service
  kubectl get pods --selector="run=load-balancer-example" --output=wide
  kubectl delete services example-service
  kubectl delete deployment hello-world




kubectl edit virtualservices.networking.istio.io myudrsim -n efrramo

 > kubectl get virtualservices.networking.istio.io -n istio-ns
NAME                            AGE
eric-udm-eecreatesubscription   1d
myrestdb                        1m




gcp
gke
aks
eks



kubectl drain worker2 --ignore-daemonsets
kubectl uncordon worker2








curl -http2-prior-knowledge -k -i -X POST -H'content-type:application/json -d'{"amfId": "AMF001", "pei" : "PEI001", "imsVoPS" : "HOMOGENEOUS_SUPPORT", "deregCallbackUri": "amf.ericsson.se:12345/dereg", "lastAMFRegistrationTS" : "2000-01-01 12:00:00", "purgeFlag" : true, "plmnId": {"mcc":"theMCC", "mnc": "theMNC"}, "urrpIndicator" : false}' 'https://udrsimhttp2:9082/nudr-dr/v1/subscription-data/imsi-00001/context-data/amf-3gpp-access'



httperf --server 172.18.18.20 --port 31382 --method POST --uri /register/kkdvk/kkdvk/kkdvk --num-calls=1000 --num-conns=10 --verbose

h2load -n 100 -c 1 -m 1  'http://172.18.18.20:31382/nudm-ee/v1/msisdn-00010/ee-subscriptions' -d eesubscription.json

h2load -n 10000 -c 1 -m 1  --h1 'http://172.18.18.20:31382/create/kkdvk' 



h2load -n 2000 -c 1 -m 1 -d ecreatesubscription.json -H ':method:POST' -H'content-type:application/json' http://172.18.18.20:31382/nudr-dr/v1/subscription-data/any-ue/group-data/ee-subscriptions

/////////////////////////
Install metric server
////////////////////////
https://github.com/kubernetes-incubator/metrics-server
////////////////////////

change metrics-service deploy

kubectl edit deployment metrics-server -nkube-system

change spec.template.spec.containers.command to

 command:
        - /metrics-server
        - --kubelet-insecure-tls
        - --kubelet-preferred-address-types=InternalIP



////////////////////////////
On existing cluster:
kubectl -n kube-system edit cm kubeadm-config
add to data->MasterConfiguration

    controllerManagerExtraArgs:
      address: 0.0.0.0
    schedulerExtraArgs:
      address: 0.0.0.0

On master node:
vim /etc/kubernetes/manifests/kube-controller-manager.yaml
vim /etc/kubernetes/manifests/kube-scheduler.yaml
and set --address=0.0.0.0
then
systemctl restart kubelet
////////////////////////////////

CHECK METRICS
////////////////////////////////
watch -n 2 -d  "kubectl top nodes;kubectl top pod;kubectl get pods -o wide"
////////////////////////////////






Para aunmentar el tamñao del storage de una VM

1.- En el ubuntu de la VM instalar  GPARTED

sudo apt-get install gparted

2.- Si tiene snapshott hacer un clon del ultimo snapshot (sin copiar los snahpshoot) y hacer el resize de la vm clon

3.- Seguir estas instrucciones:

http://derekmolloy.ie/resize-a-virtualbox-disk/

or

https://www.youtube.com/watch?v=V7ArJ1Ebd8c



kubectl exec -it eric-udm-traffic-8pzrr -n istio-system -- /bin/bash


curl http://localhost:15000/stats | grep dbproxy
curl http://localhost:15000/config_dump > kkdvk
curl -X POST http://localhost:15000/reset_counters


Install Prometheus

https://wiki.lmera.ericsson.se/wiki/PDU_UDM/UNIV/5G_Integration/ADP_Demo13


helm install ./eric-adp-5g-udm --name eric-adp-5g-udm --set tags.eric-adp-common=false --set tags.eric-data-document-database-pg=false --set tags.eric-cm-mediator=false --set tags.eric-pm-server=true --set tags.eric-pm-bulk-reporter=false --set tags.eric-data-coordinator-zk=false --set tags.eric-data-message-bus-kf=false --set tags.eric-log-shipper=false --set tags.eric-data-search-engine=false --set tags.eric-data-visualizer-gf=false --set tags.eric-data-visualizer-kb=false --set tags.eric-fh-alarm-handler=false --set tags.eric-fh-snmp-alarm-provider=false --set tags.eric-dst-jg=false


Poner como LoadBalancer

Consultar en 

http://172.18.18.20:31882/graph

Todos los microservicios que publiquen metricas en prometheus tienen que tener esta sección en su template
metadata:
  name: {{ .Values.service.name }}
  labels:
    chart: {{ .Chart.Name }}-{{ .Chart.Version | replace "+" "_" }}
  annotations:
    prometheus.io/path: "metrics"
    prometheus.io/port: {{ .Values.sidecars.pmproxy.port.http | quote }}
    prometheus.io/scrape: "true"

En caso contrario hay que añadir una sección de este estilo en el values.yaml del prometheus


      - job_name: eecreatesubscription
        static_configs:
          - targets:
            - eecreatesubscription:9500




